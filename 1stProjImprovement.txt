# ðŸš€ Crypto Pipeline Improvement Guide

## ðŸ“Š Current Project Assessment

### Strengths âœ…
- Good modular structure with separate scripts
- Uses Docker for containerization
- Includes multiple ML models (ARIMA, LSTM, Prophet)
- Has alerting via Slack
- Airflow for orchestration

### Critical Issues ðŸ”´
1. **Incomplete Docker setup** - Missing Airflow webserver/scheduler containers
2. **No error handling** - Scripts will crash on API failures
3. **Missing imports** - fetch_data.py lacks `import requests, pandas`
4. **No connection pooling** - Creates new DB connection every time
5. **No data validation** - Could store invalid/malformed data
6. **Single symbol only** - Hardcoded to BTCUSDT

---

## ðŸŽ¯ Implementation Priority

### Phase 1: Fix Critical Issues (Week 1)

#### 1. Fix Docker Compose âš¡
**Replace your docker-compose.yml** with the improved version I provided. Key changes:
- Separate webserver and scheduler services
- Proper health checks
- Init container for database setup
- Redis for caching
- Environment variable management

**Action Steps:**
```bash
# Stop current containers
docker-compose down -v

# Copy new docker-compose.yml
# Copy init-db.sh
chmod +x init-db.sh

# Create .env file from .env.example
cp .env.example .env
# Edit .env with your Slack webhook

# Rebuild and start
docker-compose up --build
```

#### 2. Update All Python Scripts
Replace your scripts with the improved versions:
- âœ… **fetch_data.py** - Added retry logic, error handling, multiple symbols
- âœ… **store_data.py** - Connection pooling, helper functions
- âœ… **transform_data.py** - Technical indicators, data cleaning
- âœ… **detect_anomalies.py** - Multiple detection methods, better accuracy

#### 3. Fix Requirements
Update `requirements.txt` with version-pinned dependencies.

---

### Phase 2: Enhance Monitoring (Week 2)

#### 4. Add Logging Configuration
Create `logging_config.py`:
```python
import logging
import sys
from pythonjsonlogger import jsonlogger

def setup_logging(log_level='INFO'):
    logger = logging.getLogger()
    logger.setLevel(log_level)
    
    # Console handler
    handler = logging.StreamHandler(sys.stdout)
    formatter = jsonlogger.JsonFormatter(
        '%(asctime)s %(name)s %(levelname)s %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger
```

#### 5. Add Prometheus Metrics
Create `metrics.py`:
```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Metrics
api_requests = Counter('api_requests_total', 'Total API requests', ['status'])
api_latency = Histogram('api_request_duration_seconds', 'API request latency')
anomalies_detected = Counter('anomalies_detected_total', 'Total anomalies')
current_price = Gauge('current_btc_price', 'Current BTC price')

# Usage in fetch_data.py
@api_latency.time()
def fetch_with_metrics():
    try:
        data = fetch_binance_price()
        api_requests.labels(status='success').inc()
        current_price.set(data['price'].iloc[0])
        return data
    except Exception as e:
        api_requests.labels(status='error').inc()
        raise
```

#### 6. Create Monitoring Dashboard
Add Grafana to `docker-compose.yml`:
```yaml
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
```

---

### Phase 3: Advanced Features (Week 3-4)

#### 7. Add Multiple Cryptocurrencies
Update DAGs to process multiple symbols:
```python
SYMBOLS = ["BTCUSDT", "ETHUSDT", "BNBUSDT", "ADAUSDT", "SOLUSDT"]

def fetch_all_symbols(**context):
    for symbol in SYMBOLS:
        df = fetch_binance_price(symbol)
        store_data(df)
```

#### 8. Implement Model Versioning
Create `models/` directory with versioning:
```
models/
â”œâ”€â”€ arima/
â”‚   â”œâ”€â”€ v1.0_20240101.pkl
â”‚   â””â”€â”€ v1.1_20240115.pkl
â”œâ”€â”€ lstm/
â”‚   â””â”€â”€ v1.0_20240101.h5
â””â”€â”€ model_registry.json
```

Track model performance:
```python
{
    "model_type": "ARIMA",
    "version": "1.0",
    "trained_at": "2024-01-01",
    "mae": 125.5,
    "rmse": 250.2,
    "symbol": "BTCUSDT"
}
```

#### 9. Add Data Quality Checks
Create `data_quality.py`:
```python
from great_expectations import DataContext

def validate_price_data(df):
    expectations = {
        'price': {'min': 0, 'max': 1000000},
        'timestamp': {'not_null': True},
        'symbol': {'in': VALID_SYMBOLS}
    }
    
    # Run validations
    results = run_expectations(df, expectations)
    
    if not results.success:
        send_slack_alert(f"Data quality issue: {results.failed_checks}")
        raise ValueError("Data quality validation failed")
```

#### 10. Real-time Streaming with WebSocket
Create `websocket_stream.py`:
```python
import websocket
import json

def on_message(ws, message):
    data = json.loads(message)
    price = float(data['p'])
    
    # Store to buffer
    price_buffer.append({
        'symbol': data['s'],
        'price': price,
        'timestamp': datetime.now()
    })
    
    # Batch insert every 10 records
    if len(price_buffer) >= 10:
        store_batch(price_buffer)
        price_buffer.clear()

ws = websocket.WebSocketApp(
    "wss://stream.binance.com:9443/ws/btcusdt@trade",
    on_message=on_message
)
ws.run_forever()
```

---

### Phase 4: Production Ready (Week 5-6)

#### 11. Add Testing
Create `tests/` directory:
```
tests/
â”œâ”€â”€ test_fetch_data.py
â”œâ”€â”€ test_transform_data.py
â”œâ”€â”€ test_anomaly_detection.py
â””â”€â”€ test_integration.py
```

Example test:
```python
import pytest
from scripts.fetch_data import fetch_binance_price

def test_fetch_valid_symbol():
    df = fetch_binance_price("BTCUSDT")
    assert not df.empty
    assert 'price' in df.columns
    assert df['price'].iloc[0] > 0

def test_fetch_invalid_symbol():
    with pytest.raises(BinanceAPIError):
        fetch_binance_price("INVALID")
```

Run tests:
```bash
pytest tests/ --cov=scripts --cov-report=html
```

#### 12. Add CI/CD Pipeline
Create `.github/workflows/ci.yml`:
```yaml
name: CI Pipeline

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.10
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest tests/
      - name: Lint code
        run: |
          black --check .
          flake8 .
```

#### 13. Add Backup Strategy
Create `backup_data.py`:
```python
def backup_to_s3():
    # Export data to CSV
    query = "SELECT * FROM crypto_prices WHERE timestamp >= NOW() - INTERVAL '7 days'"
    df = pd.read_sql(query, engine)
    
    # Upload to S3
    filename = f"backup_{datetime.now().strftime('%Y%m%d')}.csv"
    df.to_csv(f"s3://your-bucket/backups/{filename}")

# Add to DAG
backup_task = PythonOperator(
    task_id='backup_data',
    python_callable=backup_to_s3,
    schedule_interval='@daily'
)
```

#### 14. Add Model Retraining Pipeline
Create `retrain_models.py`:
```python
def retrain_all_models(**context):
    # Get last 30 days of data
    df = get_historical_data(hours=720)
    
    # Train ARIMA
    arima = train_arima(df)
    save_model(arima, 'arima', version='1.1')
    
    # Train LSTM
    lstm = train_lstm(df)
    save_model(lstm, 'lstm', version='1.1')
    
    # Evaluate and compare
    performance = evaluate_models([arima, lstm])
    
    # Deploy best model
    if performance['lstm']['rmse'] < performance['arima']['rmse']:
        deploy_model('lstm', '1.1')
```

---

## ðŸ“ˆ Performance Optimizations

### Database Optimization
```sql
-- Add indexes
CREATE INDEX idx_crypto_prices_symbol_timestamp 
ON crypto_prices(symbol, timestamp DESC);

CREATE INDEX idx_anomalies_detected_at 
ON anomalies(detected_at DESC);

-- Partition large tables
CREATE TABLE crypto_prices_2024_01 PARTITION OF crypto_prices
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

### Caching Strategy
```python
import redis
from functools import wraps

redis_client = redis.Redis(host='redis', port=6379)

def cache_result(ttl=300):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            key = f"{func.__name__}:{args}:{kwargs}"
            
            # Check cache
            cached = redis_client.get(key)
            if cached:
                return pickle.loads(cached)
            
            # Execute and cache
            result = func(*args, **kwargs)
            redis_client.setex(key, ttl, pickle.dumps(result))
            return result
        return wrapper
    return decorator

@cache_result(ttl=60)
def get_latest_price(symbol):
    return fetch_binance_price(symbol)
```

---

## ðŸ”’ Security Improvements

### 1. Use Secrets Management
```yaml
# docker-compose.yml
services:
  airflow:
    secrets:
      - db_password
      - slack_webhook

secrets:
  db_password:
    file: ./secrets/db_password.txt
  slack_webhook:
    file: ./secrets/slack_webhook.txt
```

### 2. Add API Rate Limiting
```python
from ratelimit import limits, sleep_and_retry

@sleep_and_retry
@limits(calls=1200, period=60)  # 1200 calls per minute
def fetch_with_rate_limit(symbol):
    return fetch_binance_price(symbol)
```

### 3. Input Validation
```python
from pydantic import BaseModel, validator

class PriceData(BaseModel):
    symbol: str
    price: float
    timestamp: datetime
    
    @validator('price')
    def price_must_be_positive(cls, v):
        if v <= 0:
            raise ValueError('Price must be positive')
        return v
    
    @validator('symbol')
    def symbol_must_be_valid(cls, v):
        if not v.endswith('USDT'):
            raise ValueError('Only USDT pairs supported')
        return v
```

---

## ðŸ“š Documentation Improvements

### Add API Documentation
Create `docs/API.md` with endpoints and usage:
```markdown
# API Documentation

## Endpoints

### GET /api/v1/price/{symbol}
Get latest price for a symbol

**Parameters:**
- symbol: Trading pair (e.g., BTCUSDT)

**Response:**
```json
{
  "symbol": "BTCUSDT",
  "price": 50000.00,
  "timestamp": "2024-01-01T12:00:00Z"
}
```
```

### Add Architecture Diagram
Create visual documentation using Mermaid:
```mermaid
graph TD
    A[Binance API] -->|Fetch| B[Data Ingestion]
    B --> C[Transform & Clean]
    C --> D[PostgreSQL]
    D --> E[Anomaly Detection]
    D --> F[Price Forecasting]
    E -->|Alert| G[Slack]
    F --> H[Model Storage]
```

---

## ðŸŽ“ Learning Resources

- **Airflow Best Practices**: https://airflow.apache.org/docs/
- **Time Series Forecasting**: Prophet vs ARIMA vs LSTM comparison
- **Anomaly Detection**: Isolation Forest deep dive
- **Docker Optimization**: Multi-stage builds, layer caching
- **PostgreSQL Performance**: Indexing, partitioning, vacuuming

---

## âœ… Quick Start Checklist

- [ ] Replace docker-compose.yml
- [ ] Add init-db.sh script
- [ ] Update all Python scripts
- [ ] Create .env file with Slack webhook
- [ ] Update requirements.txt
- [ ] Run `docker-compose up --build`
- [ ] Access Airflow at localhost:8080
- [ ] Enable and test DAGs
- [ ] Check PostgreSQL data: `docker exec -it postgres psql -U user -d crypto`
- [ ] Verify Slack alerts are working
- [ ] Add tests for critical functions
- [ ] Set up monitoring dashboard
- [ ] Document any custom configurations

---

## ðŸš¨ Common Issues & Solutions

**Issue**: Airflow webserver not starting
**Solution**: Check logs with `docker logs <container_id>`, ensure init container completed

**Issue**: Cannot connect to PostgreSQL
**Solution**: Verify connection string, check if postgres container is healthy

**Issue**: Slack alerts not sending
**Solution**: Verify webhook URL in .env, test with curl

**Issue**: Models not training
**Solution**: Ensure sufficient historical data (>30 days recommended)

---

## ðŸ“ž Next Steps

1. Implement Phase 1 fixes immediately
2. Test thoroughly in development
3. Set up monitoring before production
4. Gradually add Phase 2-4 features
5. Document all customizations
6. Set up alerting for system health
7. Plan for scalability (Kubernetes, distributed processing)

Good luck! ðŸš€